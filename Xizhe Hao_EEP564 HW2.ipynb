{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxzequ8BD_TS"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N_3flRvD_TT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "# Constants\n",
        "CANVAS_SIZE = 28  # Output rasterized image size (28x28)\n",
        "NUM_EXAMPLES_PER_DIGIT = 30\n",
        "NUM_CLASSES = 10  # Digits 0-9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtfjupmVD_TT"
      },
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "First, we need to extract and parse the JSON files from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmGPiNj0D_TT",
        "outputId": "634c5799-a5e6-4dce-d230-70b4eabe1300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/EEP564_HW2_Data.zip\n",
            "replace /content/EEP564_HW2_Data/AaronSong-0.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-5.json  \n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/AaronSong-9.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-5.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Anchor-9.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-5.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Dorothy-9.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-5.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Eyan-9.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-5.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/hsin-9.json  \n",
            "  inflating: /content/EEP564_HW2_Data/JiaxinZhang-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/JiaxinZhang-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/JiaxinZhang-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/JiaxinZhang-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/JiaxinZhang-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/JiaxinZhang-5.json  \n",
            " extracting: /content/EEP564_HW2_Data/JiaxinZhang-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/JiaxinZhang-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/JiaxinZhang-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/JiaxinZhang-9.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-5.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Junfeng-9.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-5.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/KunYang-9.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-5.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/yirulin-9.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-0.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-1.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-2.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-3.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-4.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-5.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-6.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-7.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-8.json  \n",
            "  inflating: /content/EEP564_HW2_Data/Zeyuan_Zhao-9.json  \n"
          ]
        }
      ],
      "source": [
        "# Set this to data directory\n",
        "\n",
        "!unzip /content/EEP564_HW2_Data.zip -d /content/\n",
        "DATA_DIR = \"./EEP564_HW2_Data\"\n",
        "\n",
        "def load_and_parse_json_files():\n",
        "    \"\"\"\n",
        "    Load and parse all JSON files in the data directory.\n",
        "    Returns a dictionary with digit labels as keys and lists of examples as values.\n",
        "    \"\"\"\n",
        "    digit_examples = {str(i): [] for i in range(10)}\n",
        "\n",
        "    # List all JSON files in the data directory\n",
        "    json_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.json')]\n",
        "\n",
        "    for json_file in json_files:\n",
        "        file_path = os.path.join(DATA_DIR, json_file)\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "                # Each file is a dictionary with a 'strokes' key\n",
        "                # The 'strokes' key contains a list of examples\n",
        "                if 'strokes' in data and isinstance(data['strokes'], list):\n",
        "                    for example in data['strokes']:\n",
        "                        if isinstance(example, dict):\n",
        "                            label = example.get('label')\n",
        "                            stroke_points = example.get('strokePoints', [])\n",
        "\n",
        "                            if label is not None and stroke_points and label in digit_examples:\n",
        "                                digit_examples[label].append(stroke_points)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {json_file}: {e}\")\n",
        "\n",
        "    # Print statistics about the dataset\n",
        "    for digit, examples in digit_examples.items():\n",
        "        print(f\"Digit {digit}: {len(examples)} examples\")\n",
        "\n",
        "    return digit_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko4dleVUD_TT"
      },
      "source": [
        "## Rasterization Function\n",
        "\n",
        "This function converts stroke points to rasterized images."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Z_PtEoGFHrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG5CUF2ED_TU"
      },
      "outputs": [],
      "source": [
        "def rasterize_stroke_points(stroke_points, canvas_size=CANVAS_SIZE):\n",
        "    \"\"\"\n",
        "    Convert stroke points to a rasterized image.\n",
        "\n",
        "    Args:\n",
        "        stroke_points: List of (x, y) coordinates\n",
        "        canvas_size: Size of the output canvas (square)\n",
        "\n",
        "    Returns:\n",
        "        A numpy array of shape (canvas_size, canvas_size) with the rasterized digit\n",
        "    \"\"\"\n",
        "    # Create an empty canvas\n",
        "    canvas = np.zeros((canvas_size, canvas_size), dtype=np.float32)\n",
        "\n",
        "    # Normalize coordinates to canvas size\n",
        "    # The original coordinates are in range [-1, 1], we need to map to [0, canvas_size-1]\n",
        "    for point in stroke_points:\n",
        "        x, y = point['x'], point['y']\n",
        "\n",
        "        # Map from [-1, 1] to [0, canvas_size-1]\n",
        "        pixel_x = int((x + 1) * (canvas_size - 1) / 2)\n",
        "        pixel_y = int((y + 1) * (canvas_size - 1) / 2)\n",
        "\n",
        "        # Ensure coordinates are within canvas bounds\n",
        "        pixel_x = max(0, min(pixel_x, canvas_size - 1))\n",
        "        pixel_y = max(0, min(pixel_y, canvas_size - 1))\n",
        "\n",
        "        # Set the pixel value to 1 (white)\n",
        "        canvas[pixel_y, pixel_x] = 1.0\n",
        "\n",
        "    return canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjX0sGnnD_TU"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "Prepare a dataset with 30 examples per digit (0-9)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8evRQeG9D_TU",
        "outputId": "27d524ed-5f36-4632-9d5f-23c28cb74c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset...\n",
            "Digit 0: 24 examples\n",
            "Digit 1: 23 examples\n",
            "Digit 2: 18 examples\n",
            "Digit 3: 19 examples\n",
            "Digit 4: 17 examples\n",
            "Digit 5: 20 examples\n",
            "Digit 6: 21 examples\n",
            "Digit 7: 20 examples\n",
            "Digit 8: 20 examples\n",
            "Digit 9: 21 examples\n",
            "Warning: Only 24 examples found for digit 0. Need 30.\n",
            "Warning: Only 23 examples found for digit 1. Need 30.\n",
            "Warning: Only 18 examples found for digit 2. Need 30.\n",
            "Warning: Only 19 examples found for digit 3. Need 30.\n",
            "Warning: Only 17 examples found for digit 4. Need 30.\n",
            "Warning: Only 20 examples found for digit 5. Need 30.\n",
            "Warning: Only 21 examples found for digit 6. Need 30.\n",
            "Warning: Only 20 examples found for digit 7. Need 30.\n",
            "Warning: Only 20 examples found for digit 8. Need 30.\n",
            "Warning: Only 21 examples found for digit 9. Need 30.\n",
            "Dataset prepared: (300, 28, 28, 1), (300, 10)\n"
          ]
        }
      ],
      "source": [
        "def prepare_dataset():\n",
        "    \"\"\"\n",
        "    Prepare a dataset with 30 examples per digit (0-9).\n",
        "\n",
        "    Returns:\n",
        "        X: numpy array of shape (300, 28, 28, 1) - rasterized images\n",
        "        y: numpy array of shape (300, 10) - one-hot encoded labels\n",
        "    \"\"\"\n",
        "    # Load and parse JSON files\n",
        "    digit_examples = load_and_parse_json_files()\n",
        "\n",
        "    # Prepare arrays for data and labels\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # For each digit (0-9)\n",
        "    for digit in range(10):\n",
        "        digit_str = str(digit)\n",
        "        examples = digit_examples.get(digit_str, [])\n",
        "\n",
        "        # Ensure we have at least 30 examples per digit\n",
        "        if len(examples) < NUM_EXAMPLES_PER_DIGIT:\n",
        "            print(f\"Warning: Only {len(examples)} examples found for digit {digit}. Need {NUM_EXAMPLES_PER_DIGIT}.\")\n",
        "            # If we don't have enough examples, we'll use what we have and duplicate some\n",
        "            if len(examples) > 0:  # Avoid division by zero\n",
        "                examples = examples * (NUM_EXAMPLES_PER_DIGIT // len(examples) + 1)\n",
        "            else:\n",
        "                print(f\"Error: No examples found for digit {digit}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "        # Select 30 examples for this digit\n",
        "        selected_examples = examples[:NUM_EXAMPLES_PER_DIGIT]\n",
        "\n",
        "        # Rasterize each example and add to dataset\n",
        "        for example in selected_examples:\n",
        "            rasterized = rasterize_stroke_points(example)\n",
        "            X.append(rasterized)\n",
        "            y.append(digit)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Reshape X to include channel dimension: (300, 28, 28) -> (300, 28, 28, 1)\n",
        "    X = X.reshape(-1, CANVAS_SIZE, CANVAS_SIZE, 1)\n",
        "\n",
        "    # Convert labels to one-hot encoding (categorical) to match the model's output\n",
        "    y_categorical = tf.keras.utils.to_categorical(y, num_classes=NUM_CLASSES)\n",
        "\n",
        "    return X, y_categorical\n",
        "\n",
        "# Prepare the dataset\n",
        "print(\"Preparing dataset...\")\n",
        "X, y = prepare_dataset()\n",
        "print(f\"Dataset prepared: {X.shape}, {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7nd9pqBD_TU"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Randomly select 1 example per class (0-9) and visualize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "J5AdGtkMD_TU",
        "outputId": "dd47f32c-5a5c-4890-ada4-d7daadf801fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizing examples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def visualize_examples(X, y):\n",
        "    \"\"\"\n",
        "    Randomly select 1 example per class (0-9) and visualize them.\n",
        "\n",
        "    Args:\n",
        "        X: numpy array of shape (n_samples, 28, 28, 1) - rasterized images\n",
        "        y: numpy array of shape (n_samples, 10) - one-hot encoded labels\n",
        "    \"\"\"\n",
        "    # Convert one-hot encoded labels back to integers for easier handling\n",
        "    y_indices = np.argmax(y, axis=1)\n",
        "\n",
        "    # Create a figure with 10 subplots (1 per digit)\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # For each digit (0-9)\n",
        "    for digit in range(10):\n",
        "        # Find all examples of this digit\n",
        "        digit_indices = np.where(y_indices == digit)[0]\n",
        "\n",
        "        if len(digit_indices) == 0:\n",
        "            print(f\"Warning: No examples found for digit {digit}. Skipping visualization.\")\n",
        "            axes[digit].text(0.5, 0.5, f\"No examples for digit {digit}\",\n",
        "                            horizontalalignment='center', verticalalignment='center')\n",
        "            axes[digit].set_title(f'Digit: {digit} (missing)')\n",
        "            axes[digit].axis('off')\n",
        "            continue\n",
        "\n",
        "        # Convert numpy array to list for random.choice\n",
        "        digit_indices_list = digit_indices.tolist()\n",
        "\n",
        "        # Randomly select one example\n",
        "        random_idx = random.choice(digit_indices_list)\n",
        "\n",
        "        # Get the image\n",
        "        img = X[random_idx, :, :, 0]  # Remove channel dimension\n",
        "\n",
        "        # Plot the image\n",
        "        axes[digit].imshow(img, cmap='gray')\n",
        "        axes[digit].set_title(f'Digit: {digit}')\n",
        "        axes[digit].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize examples\n",
        "print(\"Visualizing examples...\")\n",
        "visualize_examples(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmnmNRa8D_TU"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "Define the model architecture as in Lab 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw5JEg62D_TU"
      },
      "outputs": [],
      "source": [
        "def make_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Create the model architecture as defined in Lab 6.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Shape of the input images\n",
        "        num_classes: Number of output classes\n",
        "\n",
        "    Returns:\n",
        "        A Keras model\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
        "    x = layers.Conv2D(16, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    activation = \"softmax\"\n",
        "    units = num_classes\n",
        "\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rlzCnTCD_TU"
      },
      "source": [
        "## Load and Fine-tune Model\n",
        "\n",
        "Load the model from Lab 6 (or create a new one with the same architecture if not available), freeze all layers except the final Dense layer, and fine-tune it on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yk0gLgPfD_TU",
        "outputId": "e41d09e8-4a70-41e5-81d8-9a04b3517dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and fine-tuning model...\n",
            "Warning: model.keras not found. Creating a new model with the Lab 6 architecture.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,394\u001b[0m (95.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,394</span> (95.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,170\u001b[0m (94.41 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,170</span> (94.41 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.0750 - loss: 2.3026 - val_accuracy: 0.0500 - val_loss: 2.3040\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1133 - loss: 2.3025 - val_accuracy: 0.0333 - val_loss: 2.3045\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1414 - loss: 2.3018 - val_accuracy: 0.0333 - val_loss: 2.3053\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1260 - loss: 2.3014 - val_accuracy: 0.0333 - val_loss: 2.3061\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1130 - loss: 2.3023 - val_accuracy: 0.0333 - val_loss: 2.3068\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.1069 - loss: 2.3016 - val_accuracy: 0.0333 - val_loss: 2.3075\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1425 - loss: 2.3006 - val_accuracy: 0.0333 - val_loss: 2.3084\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1032 - loss: 2.3017 - val_accuracy: 0.0333 - val_loss: 2.3089\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1189 - loss: 2.3009 - val_accuracy: 0.0333 - val_loss: 2.3097\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1126 - loss: 2.3005 - val_accuracy: 0.0333 - val_loss: 2.3104\n"
          ]
        }
      ],
      "source": [
        "def load_and_fine_tune_model(X, y):\n",
        "    \"\"\"\n",
        "    Load the model from Lab 6, freeze all layers except the final Dense layer,\n",
        "    and fine-tune it on the dataset.\n",
        "\n",
        "    Args:\n",
        "        X: numpy array of shape (n_samples, 28, 28, 1) - rasterized images\n",
        "        y: numpy array of shape (n_samples, 10) - one-hot encoded labels\n",
        "\n",
        "    Returns:\n",
        "        The fine-tuned model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try to load the saved model from Lab 6\n",
        "        model = keras.models.load_model('model.keras')\n",
        "        print(\"Successfully loaded model.keras from Lab 6\")\n",
        "    except:\n",
        "        # If the model file is not found, create a new model with the same architecture\n",
        "        print(\"Warning: model.keras not found. Creating a new model with the Lab 6 architecture.\")\n",
        "        model = make_model(input_shape=(28, 28, 1), num_classes=10)\n",
        "        model.compile(\n",
        "            optimizer=\"adam\",\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Freeze all layers except the final Dense layer\n",
        "    for layer in model.layers[:-1]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Ensure the final layer is trainable\n",
        "    model.layers[-1].trainable = True\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Fine-tune the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Load and fine-tune the model\n",
        "print(\"Loading and fine-tuning model...\")\n",
        "model = load_and_fine_tune_model(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0HVaAWjD_TU"
      },
      "source": [
        "## Evaluate Model\n",
        "\n",
        "Evaluate the fine-tuned model and generate a classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuyoyRLGD_TU",
        "outputId": "1eed5d62-0837-4c79-c0a0-c516f8ee7a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model...\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        30\n",
            "           1       0.00      0.00      0.00        30\n",
            "           2       0.00      0.00      0.00        30\n",
            "           3       0.00      0.00      0.00        30\n",
            "           4       0.10      1.00      0.18        30\n",
            "           5       0.00      0.00      0.00        30\n",
            "           6       0.00      0.00      0.00        30\n",
            "           7       0.00      0.00      0.00        30\n",
            "           8       0.00      0.00      0.00        30\n",
            "           9       0.00      0.00      0.00        30\n",
            "\n",
            "    accuracy                           0.10       300\n",
            "   macro avg       0.01      0.10      0.02       300\n",
            "weighted avg       0.01      0.10      0.02       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, X, y):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the dataset and print the classification report.\n",
        "\n",
        "    Args:\n",
        "        model: The fine-tuned model\n",
        "        X: numpy array of shape (n_samples, 28, 28, 1) - rasterized images\n",
        "        y: numpy array of shape (n_samples, 10) - one-hot encoded labels\n",
        "    \"\"\"\n",
        "    # Predict classes\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y, axis=1)\n",
        "\n",
        "    # Print classification report\n",
        "    report = classification_report(y_true_classes, y_pred_classes)\n",
        "    print(report)\n",
        "\n",
        "    return report\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating model...\")\n",
        "report = evaluate_model(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFYIa6PdD_TV"
      },
      "source": [
        "## Save the Fine-tuned Model\n",
        "\n",
        "Save the fine-tuned model for later use in Arduino deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkwQX7ZiD_TV",
        "outputId": "8721468d-b5a0-422b-f504-f4e714421cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model saved to fine_tuned_model.keras\n"
          ]
        }
      ],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save('fine_tuned_model.keras')\n",
        "print(\"Fine-tuned model saved to fine_tuned_model.keras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('fine_tuned_model.keras')\n",
        "# Convert with int8 quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "\n",
        "# Representative dataset with correct input shape\n",
        "def representative_dataset():\n",
        "    # Your model expects (batch_size, 28, 28, 1) - MNIST-like images\n",
        "    for _ in range(100):\n",
        "        # Generate random data matching your input shape\n",
        "        yield [np.random.random((1, 28, 28, 1)).astype(np.float32)]\n",
        "\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "# Convert\n",
        "tflite_model_int8 = converter.convert()\n",
        "\n",
        "# Save\n",
        "with open('model_int8.tflite', 'wb') as f:\n",
        "    f.write(tflite_model_int8)\n",
        "\n",
        "print(\"Int8 quantized model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPGd-VzFIoXX",
        "outputId": "a120bfe6-c6bb-489c-aa2c-999354f8a91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp6b_1rle0'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer_1')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  131967269163024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967269165328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163098768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163099152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163090320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163099344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163090704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163096080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163095888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163096656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163098384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163096272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163096848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163093392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163093200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163093968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163095120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163093584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163092048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  131967163100112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Int8 quantized model saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}